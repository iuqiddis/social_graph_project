{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The New York Social Graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to get familar with web scraping, data cleaning, and eventually making a graph network of social connections.\n",
    "\n",
    "[New York Social Diary](https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/) is a kind-of  voyeuristic view into New York's socially well-to-do. The data forms a natural social graph for New York's social elite, and such is an interesting dataset to play with.  An example is the this page of a [run-of-the-mill holiday party](https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures/2014/holiday-dinners-and-doers). Please note that these links point to the internet archive, as the original website has recently removed most of its archives. Many of the images no longer load, but all the HTML is still there.\n",
    "\n",
    "For our purpose, notice that the photos have carefully annotated captions labeling those that appear in the photos.  We can think of this as implicitly implying a social graph: there is a connection between two individuals if they appear in a picture together.\n",
    "\n",
    "For this project, I'll assemble the social graph from photo captions for parties dated December 1, 2014, and before.  Using this graph, I'll make guesses at the most popular socialites, the most influential people, and the most tightly coupled pairs.\n",
    "\n",
    "I will attack the project in three phases:\n",
    "1. Get a list of all the photo pages to be analyzed.\n",
    "2. Parse all of the captions on a sample page.\n",
    "3. Parse all of the captions on all pages, and assemble the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase One\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to crawl the data. I want photos from parties on or before December 1st, 2014. I'll get the URL's and dates from the [Party Pictures Archive](https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures) to see a list of (party) pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import dill\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use Python [Requests](http://docs.python-requests.org/en/master/) to download the HTML pages, and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) to process the HTML.  I'll start by getting the [first page](https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures). I added the user-agent as requests was returning a ClientError otherwise ([stackoverflow post](https://stackoverflow.com/questions/35137724/python-requests-client-error-bad-request-but-works-after-website-has-been-open))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soupify(wbpage):\n",
    "    u_a = 'Mozilla/5.0 (X11; Linux x86_64)'\n",
    "    page = requests.get(wbpage, headers={\"USER-AGENT\":u_a})\n",
    "    print(f'status code: {page.status_code}')\n",
    "    _soup = BeautifulSoup(page.text, 'lxml')\n",
    "    return _soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status code: 200\n"
     ]
    }
   ],
   "source": [
    "wbsite = 'https://web.archive.org/web/20190206202736/http://www.newyorksocialdiary.com/party-pictures'\n",
    "soup = soupify(wbsite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page has links to 50 party pages. Look at the structure of the page and determine how to isolate those links.  Your browser's developer tools (usually `Cmd`-`Option`-`I` on Mac, `Ctrl`-`Shift`-`I` on others) offer helpful tools to explore the structure of the HTML page.\n",
    "\n",
    "Once you have found a pattern, use BeautifulSoup's [select](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors) or [find_all](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find) methods to get those elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getlinks(_soup):\n",
    "    link_divs = _soup.select('div.views-row span.field-content a') # The a only selects the links span\n",
    "    reg_str = '<a href=\"(.*)\">'\n",
    "    print('Total links in tags:', len(link_divs))\n",
    "    \n",
    "    _cml =  [] # completed links\n",
    "    for link in link_divs:\n",
    "        cmp = re.findall(reg_str, str(link))\n",
    "        #regex library takes only strins, so converted list object to string.\n",
    "        _cml.append('https://web.archive.org{}'.format(cmp[0]))\n",
    "\n",
    "    return _cml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total links in tags: 50\n",
      "The length of this URL list is: 50 \n",
      "\n",
      "First 5  urls are:\n",
      "\n",
      "https://web.archive.org/web/20190206202736/http://www.newyorksocialdiary.com/party-pictures/2019/jams-and-journeys\n",
      "https://web.archive.org/web/20190206202736/http://www.newyorksocialdiary.com/party-pictures/2019/hanging-out-the-ham\n",
      "https://web.archive.org/web/20190206202736/http://www.newyorksocialdiary.com/party-pictures/2018/gold-medal-winners\n",
      "https://web.archive.org/web/20190206202736/http://www.newyorksocialdiary.com/party-pictures/2018/timeless-traditions\n",
      "https://web.archive.org/web/20190206202736/http://www.newyorksocialdiary.com/party-pictures/2018/the-order-of-st-john-of-jerusalems-annual-charity-holiday-gala\n"
     ]
    }
   ],
   "source": [
    "cml = getlinks(soup)\n",
    "print('The length of this URL list is:', len(cml), '\\n')\n",
    "print('First 5  urls are:\\n')\n",
    "for i in cml[0:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at that first link.  Figure out how to extract the URL of the link, as well as the date.  You probably want to use `datetime.strptime`.  See the [format codes for dates](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior) for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdates(_soup):\n",
    "    link_divs = _soup.select('div.views-row span.field-content')  \n",
    "    # Unlike getlinks(), cannot exclusively select dates\n",
    "    print('Total URL + dates selected:', len(link_divs))\n",
    "    ti = int(len(link_divs)/2)\n",
    "    link_divs2 = [i for i in range(ti)]\n",
    "    for i in link_divs2:\n",
    "        link_divs2[i] = link_divs[i*2+1]\n",
    "\n",
    "    print('Length of just dates list is:', len(link_divs2))\n",
    "\n",
    "    reg_str = '<span class=\"field-content\">\\w*,(.*)<\\/span>'\n",
    "    cmp= list(range(0,ti))\n",
    "    cmt= list(range(0,ti))\n",
    "\n",
    "    for i in range(0,len(link_divs2)): #needed the range function as link_divs2 is not a list\n",
    "        cmp[i] = re.findall(reg_str, str(link_divs2[i]))\n",
    "        cmt[i] = dt.strptime(str(cmp[i]), \"[' %B %d, %Y']\")\n",
    "\n",
    "\n",
    "    return cmp,cmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to check that it works as you expected.\n",
    "\n",
    "Once that's working, let's write another function to parse all of the links on a page.  Thinking ahead, we can make it take a Requests [Response](http://docs.python-requests.org/en/master/api/#requests.Response) object and do the BeautifulSoup parsing within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URL + dates selected: 40\n",
      "Length of just dates list is: 20\n",
      "The length of this \"date\" list is: 20 \n",
      "\n",
      "First 5 dates are:\n",
      " [[' April 2, 2007'], [' March 29, 2007'], [' March 28, 2007'], [' March 27, 2007'], [' March 26, 2007']] \n",
      "\n",
      "First 5 dates are:\n",
      " [datetime.datetime(2007, 4, 2, 0, 0), datetime.datetime(2007, 3, 29, 0, 0), datetime.datetime(2007, 3, 28, 0, 0), datetime.datetime(2007, 3, 27, 0, 0), datetime.datetime(2007, 3, 26, 0, 0)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmp2, cmt = getdates(soup)\n",
    "print('The length of this \"date\" list is:', len(cmp2), '\\n')\n",
    "print('First 5 dates are:\\n', cmp2[0:5], '\\n')\n",
    "print('First 5 dates are:\\n', cmt[0:5], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should be ready to get all of the party URLs.  Click through a few of the index pages to determine how the URL changes.  Figure out a strategy to visit all of them.\n",
    "\n",
    "HTTP requests are generally IO-bound.  This means that most of the time is spent waiting for the remote server to respond.  If you use `requests` directly, you can only wait on one response at a time.  [requests-futures](https://github.com/ross/requests-futures) lets you wait for multiple requests at a time.  You may wish to use this to speed up the downloading process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_futures.sessions import FuturesSession\n",
    "\n",
    "link_list = []\n",
    "# You can use link_list.extend(others) to add the elements of others\n",
    "# to link_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures, URL page#: 1\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=1, URL page#: 2\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=2, URL page#: 3\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=3, URL page#: 4\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=4, URL page#: 5\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=5, URL page#: 6\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=6, URL page#: 7\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=7, URL page#: 8\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=8, URL page#: 9\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=9, URL page#: 10\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=10, URL page#: 11\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=11, URL page#: 12\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=12, URL page#: 13\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=13, URL page#: 14\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=14, URL page#: 15\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=15, URL page#: 16\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=16, URL page#: 17\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=17, URL page#: 18\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=18, URL page#: 19\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=19, URL page#: 20\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=20, URL page#: 21\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=21, URL page#: 22\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=22, URL page#: 23\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=23, URL page#: 24\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=24, URL page#: 25\n",
      "status code: 200\n",
      "Total links in tags: 50\n",
      "Total URL + dates selected: 100\n",
      "Length of just dates list is: 50\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=25, URL page#: 26\n",
      "status code: 200\n",
      "Total links in tags: 23\n",
      "Total URL + dates selected: 46\n",
      "Length of just dates list is: 23\n",
      "URL count: https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures?page=26, URL page#: 27\n",
      "status code: 200\n",
      "Total links in tags: 20\n",
      "Total URL + dates selected: 40\n",
      "Length of just dates list is: 20\n"
     ]
    }
   ],
   "source": [
    "cml, cmp2, cmt = [], [], []\n",
    "wbsite  = 'https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures'\n",
    "\n",
    "\n",
    "for i in range(0,27):\n",
    "    if i == 0:\n",
    "        wb2 = wbsite\n",
    "    else:\n",
    "        wb2 = wbsite+ f'?page={i}'\n",
    "    print(f'URL count: {wb2}, URL page#: {i+1}')\n",
    "    soup = soupify(wb2)\n",
    "\n",
    "    cml = cml + getlinks(soup)\n",
    "\n",
    "    cmpp2, cmtp = getdates(soup)\n",
    "    cmp2 = cmp2 + cmpp2\n",
    "    cmt = cmt + cmtp\n",
    "\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://web.archive.org/web/20190206202736/http://www.newyorksocialdiary.com/party-pictures/2019/jams-and-journeys\n",
      "[' January 17, 2019']\n",
      "2019-01-17 00:00:00\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "print(cml[n], cmp2[n], cmt[n], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we only want parties with dates on or before the first of December, 2014.  Let's write a function to filter our list of dates to those at or before a cutoff.  Using a keyword argument, we can put in a default cutoff, but allow us to test with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'date': cmt,\n",
    "                   'url': cml})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>mY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2014-12-01</td>\n",
       "      <td>https://web.archive.org/web/20150918040703/htt...</td>\n",
       "      <td>Dec-2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2014-11-24</td>\n",
       "      <td>https://web.archive.org/web/20150918040703/htt...</td>\n",
       "      <td>Nov-2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>2014-11-20</td>\n",
       "      <td>https://web.archive.org/web/20150918040703/htt...</td>\n",
       "      <td>Nov-2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2014-11-18</td>\n",
       "      <td>https://web.archive.org/web/20150918040703/htt...</td>\n",
       "      <td>Nov-2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2014-11-17</td>\n",
       "      <td>https://web.archive.org/web/20150918040703/htt...</td>\n",
       "      <td>Nov-2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                                url        mY\n",
       "80 2014-12-01  https://web.archive.org/web/20150918040703/htt...  Dec-2014\n",
       "81 2014-11-24  https://web.archive.org/web/20150918040703/htt...  Nov-2014\n",
       "82 2014-11-20  https://web.archive.org/web/20150918040703/htt...  Nov-2014\n",
       "83 2014-11-18  https://web.archive.org/web/20150918040703/htt...  Nov-2014\n",
       "84 2014-11-17  https://web.archive.org/web/20150918040703/htt...  Nov-2014"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['date'] <= '2014-12-01']\n",
    "df['mY'] = df['date'].dt.strftime('%b-%Y')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, you should have 1193 parties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we need to restart the notebook, we should save this information to a file.  There are many ways you could do this; here's one using `dill`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(link_list, open('nysd-links.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To restore the list, we can just load it from the file.  When the notebook is restarted, you can skip the code above and just run this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = dill.load(open('nysd-links.pkd', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpost 1: histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of party pages for the 95 months (that is, month-year pair) in the data.  Represent this histogram as a list of 95 tuples, each of the form `(\"Dec-2014\", 1)`.  Note that you can convert `datetime` objects into these sort of strings with `strftime` and the [format codes for dates](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior).\n",
    "\n",
    "Plot the histogram for yourself.  Do you see any trends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grp = df.groupby(['mY'], as_index=False)\n",
    "grp_ct = df_grp['mY'].size().tolist()\n",
    "hgram = []\n",
    "for i,j in zip(df_grp, grp_ct):\n",
    "    hgram.append((i[0], j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase Two\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this phase, we we concentrate on getting the names out of captions for a given page.  We'll start with [the benefit cocktails and dinner](https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures/2015/celebrating-the-neighborhood) for [Lenox Hill Neighborhood House](http://www.lenoxhill.org/), a neighborhood organization for the East Side.\n",
    "\n",
    "Take a look at that page.  Note that some of the text on the page is captions, but others are descriptions of the event.  Determine how to select only the captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def soupify(wbpage):\n",
    "    page = requests.get(wbpage)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    return soup\n",
    "\n",
    "soup = soupify(wbsite)\n",
    "#print(soup.prettify())\n",
    "\n",
    "def getlinks():\n",
    "    link_divs = soup.select('div.photocaption') # For graph 2 question\n",
    "\n",
    "    ti = int(len(link_divs)) # total items\n",
    "    print('total items:', ti, '\\n')\n",
    "    cmp = list(range(0, ti)) #compiled list\n",
    "    #cml = list(range(0,ti)) # completed links\n",
    "    #print(link_divs)\n",
    "    reg_str = '<div.*>(.*)<\\/div>'\n",
    "    print(\"Following is automatically generated list from captions:\\n\")\n",
    "    for i in range(0, ti):\n",
    "        cmp[i] = re.findall(reg_str, str(link_divs[i]))\n",
    "        #regular expressions library takes only strings, so converted list object to string.\n",
    "        #cml[i] = 'https://web.archive.org{}'.format(cmp[i][0])\n",
    "        print(cmp[i])\n",
    "\n",
    "    return ti, cmp\n",
    "\n",
    "ti, cmp = getlinks()\n",
    "print('\\nProcessing Round 1\\n')\n",
    "\n",
    "nms = {} #names list\n",
    "\n",
    "for i in range(ti):\n",
    "    if not cmp[i]: #added this as otherwise indexing would choke on an empty list item\n",
    "        print('cmp[',i,'] is an empty array')\n",
    "    elif len(cmp[i][0]) > 250:\n",
    "        print('Greater than 250 characters. skipping it')\n",
    "    else:\n",
    "        reg_str = '^\\s'\n",
    "        cmp[i][0] = re.sub(reg_str, '', cmp[i][0])\n",
    "\n",
    "        reg_str = '\\s$'\n",
    "        cmp[i][0] = re.sub(reg_str, '', cmp[i][0])\n",
    "\n",
    "        if ' at ' in cmp[i][0]:\n",
    "            reg_str = '\\sat\\s.*'\n",
    "            cmp[i][0] = re.sub(reg_str, '', cmp[i][0])\n",
    "\n",
    "        if '(' in cmp[i][0]:\n",
    "            print('Removed a parenthesis from ', i)\n",
    "            reg_str = '\\(.*\\)'\n",
    "            cmp[i][0] = re.sub(reg_str, '', cmp[i][0])\n",
    "\n",
    "        if 'and friend' in cmp[i][0]:\n",
    "            print(\"Removing 'and friend(s)' from this item:\", i)\n",
    "            reg_str = '\\sand\\sfriends*'\n",
    "            cmp[i][0] = re.sub(reg_str, '', cmp[i][0])\n",
    "\n",
    "\n",
    "print('\\nThis is the list after spaces and other things are removed:\\n')\n",
    "for i in range(ti):\n",
    "    print(cmp[i])\n",
    "\n",
    "print('\\nSplitting names into separate items:\\n')\n",
    "\n",
    "for i in range(0,ti): # putting each name into its own list\n",
    "    print(i)\n",
    "    if not cmp[i]: #added this as otherwise indexing would choke on an empty list item\n",
    "        print('cmp[',i,'] is an empty array')\n",
    "    elif len(cmp[i][0]) > 250:\n",
    "        print('Greater than 250 characters. skipping it')\n",
    "    elif not(',' in cmp[i][0]):\n",
    "        reg_str = '^([^\\s]+)\\s+and\\s+([^\\s]+)\\s+([^\\s]+)$' # separating 'first and first last' name format\n",
    "        if not(' and ' in cmp[i][0]): # means just a single name in this caption\n",
    "            nl = len(nms)  # nms length\n",
    "            nms[nl] = (cmp[i][0])\n",
    "        elif re.search(reg_str, cmp[i][0]):\n",
    "            rout = re.findall(reg_str, cmp[i][0])\n",
    "            nl = len(nms)  # this will change as more names get split\n",
    "            nms[i] = rout[0][0] + ' ' + rout[0][2]\n",
    "            nms[nl] = rout[0][1] + ' ' + rout[0][2]\n",
    "            print('Split this name for', i)\n",
    "        else:\n",
    "            nl = len(nms)\n",
    "            ln1, ln2 = cmp[i][0].split(' and ') #list of names 1 and 2\n",
    "            nms[nl] = ln1\n",
    "            nms[nl + 1] = ln2\n",
    "    else:\n",
    "        nl = len(nms)\n",
    "        ln = cmp[i][0].split(', ') # list of names assuming separated by comma\n",
    "        for j in range(0,len(ln)):\n",
    "            if 'and ' in ln[j]:\n",
    "                ln[j] = re.sub('and\\s', '', ln[j])\n",
    "            nms[nl + j] = ln[j]\n",
    "\n",
    "print('\\nCleaning up the list. Dangerous, be careful:\\n')\n",
    "\n",
    "def clean_things():\n",
    "\n",
    "    print('nms list before cleaning round 2:\\n', nms)\n",
    "    nml = len(nms)\n",
    "    print('\\nnms list length before round 2:', nml, '\\n')\n",
    "\n",
    "    for i in range(0,nml):\n",
    "        if 'and ' in nms[i]:\n",
    "            print(\"Removing 'and' from from of this item:\", nms[i])\n",
    "            reg_str = '^and\\s'\n",
    "            nms[i] = re.sub(reg_str, '', nms[i])\n",
    "\n",
    "        if 'with ' in nms[i]:\n",
    "            print(\"Removing 'with' from from of this item:\", nms[i])\n",
    "            reg_str = ' with '\n",
    "            nms[i], nms[nml] = nms[i].split(reg_str)\n",
    "\n",
    "        if 'Dr. ' in nms[i]:\n",
    "            print(\"Removing 'Dr. ' from this item\", nms[i])\n",
    "            reg_str = '^Dr\\.\\s'\n",
    "            nms[i] = re.sub(reg_str, '', nms[i])\n",
    "\n",
    "        if 'Mr. ' in nms[i]:\n",
    "            print(\"Removing 'Mr. ' from this item:\", nms[i])\n",
    "            reg_str = '^Mr\\.\\s'\n",
    "            nms[i] = re.sub(reg_str, '', nms[i])\n",
    "\n",
    "        if 'Mrs. ' in nms[i]:\n",
    "            print(\"Removing 'Mrs. ' from this item:\", nms[i])\n",
    "            reg_str = '^Mrs\\.\\s'\n",
    "            nms[i] = re.sub(reg_str, '', nms[i])\n",
    "\n",
    "        if 'Esq.' in nms[i]:\n",
    "            print(\"Removing 'Esq. ' from this item:\", nms[i])\n",
    "            reg_str = '\\s*Esq\\.$'\n",
    "            nms[i] = re.sub(reg_str, '', nms[i])\n",
    "\n",
    "        if ' on ' in nms[i]:\n",
    "            print(\"Removing anything after ' on ' from this item:\", nms[i])\n",
    "            reg_str = '\\son\\s.*$'\n",
    "            nms[i] = re.sub(reg_str, '', nms[i])\n",
    "    return nms\n",
    "\n",
    "nms = clean_things()\n",
    "nms = clean_things() #running again for picking up extra dirty stuff\n",
    "\n",
    "\n",
    "print('\\nnms list AFTER cleaning round 2:', nms, '\\n')\n",
    "print('nms list length AFTER cleaning round 2:', len(nms), '\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nmu = {} # unique name list\n",
    "for key, value in nms.items():\n",
    "    if value not in nmu.values():\n",
    "        nmu[key] = value\n",
    "\n",
    "\n",
    "nm_out = sorted(nms.values())\n",
    "nm_out2 = sorted(nmu.values())\n",
    "print(nm_out)\n",
    "print(nm_out2[2:102])\n",
    "#print(nmu)\n",
    "\n",
    "for i in range(len(nm_out2)):\n",
    "    print(nm_out2[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By our count, there are about 110.  But if you're off by a couple, you're probably okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert abs(len(captions) - 110) < 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encapsulate this in a function.  As with the links pages, we want to avoid downloading a given page the next time we need to run the notebook.  While we could save the files by hand, as we did before, a checkpoint library like [ediblepickle](https://pypi.python.org/pypi/ediblepickle/1.1.3) can handle this for you.  (Note, though, that you may not want to enable this until you are sure that your function is working.)\n",
    "\n",
    "You should also keep in mind that HTTP requests fail occasionally, for transient reasons.  You should plan how to detect and react to these failures.   The [retrying module](https://pypi.python.org/pypi/retrying) is one way to deal with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_captions(path):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should get the same captions as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert captions == get_captions(\"/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures/2015/celebrating-the-neighborhood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some sample captions, let's start parsing names out of those captions.  There are many ways of going about this, and we leave the details up to you.  Some issues to consider:\n",
    "\n",
    "  1. Some captions are not useful: they contain long narrative texts that explain the event.  Try to find some heuristic rules to separate captions that are a list of names from those that are not.  A few heuristics include:\n",
    "    - look for sentences (which have verbs) and as opposed to lists of nouns. For example, [`nltk` does part of speech tagging](http://www.nltk.org/book/ch05.html) but it is a little slow. There may also be heuristics that accomplish the same thing.\n",
    "    - Similarly, spaCy's [entity recognition](https://spacy.io/docs/usage/entity-recognition) could be useful here.\n",
    "    - Look for commonly repeated threads (e.g. you might end up picking up the photo credits or people such as \"a friend\").\n",
    "    - Long captions are often not lists of people.  The cutoff is subjective, but for grading purposes, *set that cutoff at 250 characters*.\n",
    "  2. You will want to separate the captions based on various forms of punctuation.  Try using `re.split`, which is more sophisticated than `string.split`. **Note**: The reference solution uses regex exclusively for name parsing.\n",
    "  3. You might find a person named \"ra Lebenthal\".  There is no one by this name.  Can anyone spot what's happening here?\n",
    "  4. This site is pretty formal and likes to say things like \"Mayor Michael Bloomberg\" after his election but \"Michael Bloomberg\" before his election.  Can you find other ('optional') titles that are being used?  They should probably be filtered out because they ultimately refer to the same person: \"Michael Bloomberg.\"\n",
    "  5. There is a special case you might find where couples are written as eg. \"John and Mary Smith\". You will need to write some extra logic to make sure this properly parses to two names: \"John Smith\" and \"Mary Smith\".\n",
    "  6. When parsing names from captions, it can help to look at your output frequently and address the problems that you see coming up, iterating until you have a list that looks reasonable. This is the approach used in the reference solution. Because we can only asymptotically approach perfect identification and entity matching, we have to stop somewhere.\n",
    "  \n",
    "**Questions worth considering:**\n",
    "  1. Who is Patrick McMullan and should he be included in the results? How would you address this?\n",
    "  2. What else could you do to improve the quality of the graph's information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: sample_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you feel that your algorithm is working well on these captions, parse all of the captions and extract all the names mentioned.  Sort them alphabetically, by first name, and return the first hundred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_names = [\"Caroline Dean\"] * 100\n",
    "\n",
    "grader.score('graph__sample_names', sample_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run this sort of test on a few other pages.  You will probably find that other pages have a slightly different HTML structure, as well as new captions that trip up your caption parser.  But don't worry if the parser isn't perfect -- just try to get the easy cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase Three\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are satisfied that your caption scraper and parser are working, run this for all of the pages.  If you haven't implemented some caching of the captions, you probably want to do this first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import dill\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "import itertools\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "########################################\n",
    "\n",
    "\n",
    "def graph3all(soup):\n",
    "\n",
    "\n",
    "    #This function is called getlinks. But it gets captions.\n",
    "    def getlinks():\n",
    "        link_divs = soup.select('td.photocaption') # For graph 2 question\n",
    "        #link_divs = soup.select('div.photocaption')  # For graph 2 question\n",
    "\n",
    "        ti = int(len(link_divs)) # total items\n",
    "        #print('total items:', ti, '\\n')\n",
    "        cmp = list(range(0, ti)) #compiled list\n",
    "        #cml = list(range(0,ti)) # completed links - just need it to use it once and save list\n",
    "        #reg_str = '<div.*>(.*)<\\/div>'\n",
    "        reg_str = '<td.*>(.*)<\\/td>'\n",
    "        #print(\"Following is automatically generated list from captions:\\n\")\n",
    "        for i in range(0, ti):\n",
    "            cmp[i] = re.findall(reg_str, str(link_divs[i]))\n",
    "            #regular expressions library takes only strings, so converted list object to string.\n",
    "            #cml[i] = 'https://web.archive.org{}'.format(cmp[i][0])\n",
    "            #print(cmp[i])\n",
    "\n",
    "        return ti, cmp\n",
    "\n",
    "    ti, cmp = getlinks()\n",
    "    #print('\\nProcessing Round 1\\n')\n",
    "\n",
    "    nms = {} #names list\n",
    "\n",
    "    for i in range(ti):\n",
    "        if not cmp[i]: #added this as otherwise indexing would choke on an empty list item\n",
    "            #print('cmp[',i,'] is an empty array')\n",
    "            continue\n",
    "        elif len(cmp[i][0]) > 250:\n",
    "            #print('Greater than 250 characters. skipping it')\n",
    "            continue\n",
    "        else:\n",
    "            reg_str = '^\\s'\n",
    "            cmp[i][0] = re.sub(reg_str, '', cmp[i][0])\n",
    "\n",
    "            reg_str = '\\s$'\n",
    "            cmp[i][0] = re.sub(reg_str, '', cmp[i][0])\n",
    "\n",
    "            if ' at ' in cmp[i][0]:\n",
    "                reg_str = '\\sat\\s.*'\n",
    "                cmp[i][0] = re.sub(reg_str, '', cmp[i][0])\n",
    "\n",
    "            if '(' in cmp[i][0]:\n",
    "                #print('Removed a parenthesis from ', i)\n",
    "                reg_str = '\\(.*\\)'\n",
    "                cmp[i][0] = re.sub(reg_str, '', cmp[i][0])\n",
    "\n",
    "            if 'and friend' in cmp[i][0]:\n",
    "                #print(\"Removing 'and friend(s)' from this item:\", i)\n",
    "                reg_str = '\\sand\\sfriends*'\n",
    "                cmp[i][0] = re.sub(reg_str, '', cmp[i][0])\n",
    "\n",
    "\n",
    "    #print('\\nThis is the list after spaces and other things are removed:\\n')\n",
    "    #for i in range(ti):\n",
    "        #print(i, cmp[i])\n",
    "\n",
    "    #print('\\nSplitting names into separate items:\\n')\n",
    "\n",
    "    for i in range(0,ti): # putting each name into its own list\n",
    "        nms[i] = {}\n",
    "        # print(i)\n",
    "        if not cmp[i]: #added this as otherwise indexing would choke on an empty list item\n",
    "            continue\n",
    "            #print('cmp[',i,'] is an empty array')\n",
    "        elif len(cmp[i][0]) > 250:\n",
    "            continue\n",
    "            #print('Greater than 250 characters. skipping it')\n",
    "        elif not(',' in cmp[i][0]):\n",
    "\n",
    "            if ' Jr.' in cmp[i][0]: #added this to deal with ,Jr. supposedly\n",
    "                #print(\"Removing ' Jr. ' from this item:\", cmp[i][0])\n",
    "                reg_str = '\\sJr\\.'\n",
    "                cmp[i][0] = re.sub(reg_str, '', cmp[i][0])\n",
    "\n",
    "            if ' MD' in cmp[i][0]:\n",
    "                #print(\"Removing 'MD ' from this item:\", cmp[i][0])\n",
    "                reg_str = '\\sMD'\n",
    "                cmp[i][0] = re.sub(reg_str, '', cmp[i][0])\n",
    "\n",
    "            if ' M.D.' in cmp[i][0]:\n",
    "                #print(\"Removing 'M.D. ' from this item:\", cmp[i][0])\n",
    "                reg_str = '\\s*M\\.D\\.'\n",
    "                cmp[i][0] = re.sub(reg_str, '', cmp[i][0])\n",
    "\n",
    "            if ' M D' in cmp[i][0]:\n",
    "                #print(\"Removing 'M.D. ' from this item:\", cmp[i][0])\n",
    "                reg_str = '\\s*M\\sD'\n",
    "                cmp[i][0] = re.sub(reg_str, '', cmp[i][0])\n",
    "\n",
    "            reg_str = '^([^\\s]+)\\s+and\\s+([^\\s]+)\\s+([^\\s]+)$' # separating 'first and first last' name format\n",
    "            if not(' and ' in cmp[i][0]): # means just a single name in this caption\n",
    "                nms[i][0] = (cmp[i][0])\n",
    "            elif re.search(reg_str, cmp[i][0]):\n",
    "                rout = re.findall(reg_str, cmp[i][0])\n",
    "                nms[i] = {}\n",
    "                nms[i][0] = rout[0][0] + ' ' + rout[0][2]\n",
    "                nms[i][1] = rout[0][1] + ' ' + rout[0][2]\n",
    "                #print('Split this first and first last name for', i)\n",
    "            else:\n",
    "                #print('Split these two names caption for', i, cmp[i])\n",
    "                ln1 = cmp[i][0].split(' and ') #list of names 1 and 2\n",
    "                nms[i] = {}\n",
    "                nms[i][0] = ln1[0]\n",
    "                nms[i][1] = ln1[1]\n",
    "        else:\n",
    "            if ' Jr.' in cmp[i][0]: #added this to deal with ,Jr. supposedly\n",
    "                #print(\"Removing ' Jr. ' from this item:\", cmp[i][0])\n",
    "                reg_str = '\\sJr\\.'\n",
    "                cmp[i][0] = re.sub(reg_str, '', cmp[i][0])\n",
    "\n",
    "            if 'MD' in cmp[i][0]:\n",
    "                #print(\"Removing 'MD ' from this item:\", cmp[i][0])\n",
    "                reg_str = '\\sMD'\n",
    "                cmp[i][0] = re.sub(reg_str, '', cmp[i][0])\n",
    "\n",
    "            if 'M.D.' in cmp[i][0]:\n",
    "                #print(\"Removing 'M.D. ' from this item:\", cmp[i][0])\n",
    "                reg_str = '\\s*M\\.D\\.'\n",
    "                cmp[i][0] = re.sub(reg_str, '', cmp[i][0])\n",
    "\n",
    "            if ' M D' in cmp[i][0]:\n",
    "                #print(\"Removing 'M.D. ' from this item:\", cmp[i][0])\n",
    "                reg_str = '\\s*M\\sD'\n",
    "                cmp[i][0] = re.sub(reg_str, '', cmp[i][0])\n",
    "\n",
    "            ln = cmp[i][0].split(', ') # list of names assuming separated by comma\n",
    "            for j in range(0,len(ln)):\n",
    "                if 'and ' in ln[j]:\n",
    "                    ln[j] = re.sub('and\\s', '', ln[j])\n",
    "                nms[i][j] = ln[j]\n",
    "                #print(nms[i][j])\n",
    "            #print('Splitting these comma separated names for', i)\n",
    "\n",
    "    #print(nms)\n",
    "\n",
    "    #print('\\nCleaning up the list. Dangerous, be careful:\\n')\n",
    "    #print('nms list before cleaning round 2:\\n', nms)\n",
    "\n",
    "    # nmi is nms item\n",
    "    def clean_things(nmi):\n",
    "\n",
    "        #nmi2 = []\n",
    "\n",
    "        if 'and ' in nmi:\n",
    "            #print(\"Removing 'and' from from of this item:\", nmi)\n",
    "            reg_str = '^and\\s'\n",
    "            nmi = re.sub(reg_str, '', nmi)\n",
    "\n",
    "        if 'with ' in nmi:\n",
    "            #print(\"Removing 'with' from from of this item:\", i, nmi)\n",
    "            reg_str = ' with '\n",
    "            nmi = nmi.split(reg_str)\n",
    "\n",
    "        if 'Mayor ' in nmi:\n",
    "            #print(\"Removing 'Mayor ' from this item\", nmi)\n",
    "            reg_str = '^Mayor\\s'\n",
    "            nmi = re.sub(reg_str, '', nmi)\n",
    "\n",
    "        if 'President ' in nmi:\n",
    "            #print(\"Removing 'President ' from this item\", nmi)\n",
    "            reg_str = '^President\\s'\n",
    "            nmi = re.sub(reg_str, '', nmi)\n",
    "\n",
    "        if 'Consul General ' in nmi:\n",
    "            #print(\"Removing 'Consul General ' from this item\", nmi)\n",
    "            reg_str = '^Consul General\\s'\n",
    "            nmi = re.sub(reg_str, '', nmi)\n",
    "\n",
    "        if 'Sir ' in nmi:\n",
    "            #print(\"Removing 'Sir ' from this item\", nmi)\n",
    "            reg_str = '^Sir\\s'\n",
    "            nmi = re.sub(reg_str, '', nmi)\n",
    "\n",
    "        if 'HRH Princess ' in nmi:\n",
    "            #print(\"Removing 'HRH Princess ' from this item\", nmi)\n",
    "            reg_str = '^HRH\\sPrincess\\s'\n",
    "            nmi = re.sub(reg_str, '', nmi)\n",
    "\n",
    "        if 'Ambassador ' in nmi:\n",
    "            #print(\"Ambassador ' from this item\", nmi)\n",
    "            reg_str = '^Ambassador\\s'\n",
    "            nmi = re.sub(reg_str, '', nmi)\n",
    "\n",
    "        if 'Dr. ' in nmi:\n",
    "            #print(\"Removing 'Dr. ' from this item\", nmi)\n",
    "            reg_str = '^Dr\\.\\s'\n",
    "            nmi = re.sub(reg_str, '', nmi)\n",
    "\n",
    "        if 'Mr. ' in nmi:\n",
    "            #print(\"Removing 'Mr. ' from this item:\", nmi)\n",
    "            reg_str = '^Mr\\.\\s'\n",
    "            nmi = re.sub(reg_str, '', nmi)\n",
    "\n",
    "        if 'Mrs. ' in nmi:\n",
    "            #print(\"Removing 'Mrs. ' from this item:\", nmi)\n",
    "            reg_str = '^Mrs\\.\\s'\n",
    "            nmi = re.sub(reg_str, '', nmi)\n",
    "\n",
    "        if 'Esq.' in nmi:\n",
    "            #print(\"Removing 'Esq. ' from this item:\", nmi)\n",
    "            reg_str = '\\s*Esq\\.$'\n",
    "            nmi = re.sub(reg_str, '', nmi)\n",
    "\n",
    "        # if 'MD' in nmi:\n",
    "        #     #print(\"Removing 'MD ' from this item:\", nmi)\n",
    "        #     reg_str = '\\sMD$'\n",
    "        #     nmi = re.sub(reg_str, '', nmi)\n",
    "\n",
    "        if 'M.D.' in nmi:\n",
    "            #print(\"Removing 'M.D. ' from this item:\", nmi)\n",
    "            reg_str = '\\s*M\\.D\\.$'\n",
    "            nmi = re.sub(reg_str, '', nmi)\n",
    "\n",
    "        if 'Jr.' in nmi:\n",
    "            #print(\"Removing 'Jr. ' from this item:\", nmi)\n",
    "            reg_str = '\\s*Jr\\.$'\n",
    "            nmi = re.sub(reg_str, '', nmi)\n",
    "\n",
    "        if ' on ' in nmi:\n",
    "            #print(\"Removing anything after ' on ' from this item:\", nmi)\n",
    "            reg_str = '\\son\\s.*$'\n",
    "            nmi = re.sub(reg_str, '', nmi)\n",
    "\n",
    "        if ' examining ' in nmi:\n",
    "            #print(\"Removing anything after ' examining ' from this item:\", nmi)\n",
    "            reg_str = '\\sexamining\\s.*$'\n",
    "            nmi = re.sub(reg_str, '', nmi)\n",
    "\n",
    "        if 'Mr.' in nmi or 'Mrs.' in nmi: #order matters, after the other Mrs. one\n",
    "            #print(\"Deleting this entry:\", i,',',nmi)\n",
    "            #del nmi\n",
    "            nmi = {}\n",
    "\n",
    "        return nmi\n",
    "\n",
    "\n",
    "    nml = len(nms)\n",
    "    for i in range(nml):\n",
    "        nmk = len(nms[i])\n",
    "        for j in range(nmk):\n",
    "            nmt = clean_things(nms[i][j]) # nm_temp\n",
    "            #print(nmt)\n",
    "            if len(nmt) == 2:\n",
    "                nmk = len(nms[i])\n",
    "                nms[i][nmk] = nmt[1]\n",
    "                nms[i][j] = nmt[0]\n",
    "            elif len(nmt) == 1:\n",
    "                nms[i][j] = nmt[0]\n",
    "            elif len(nmt) == 0:\n",
    "                del nms[i][j]\n",
    "\n",
    "\n",
    "    #print('\\nAfter the looping change:')\n",
    "\n",
    "    #for i,j in nms.items():\n",
    "        #print(i, j)\n",
    "\n",
    "    #cleaning\n",
    "    for l,i in nms.items():\n",
    "        k = 0\n",
    "        new_dict = {}\n",
    "        for j in i:\n",
    "            new_dict[k] = i[j]\n",
    "            k += 1\n",
    "        nms[l] = new_dict\n",
    "\n",
    "    #print('\\n\\n\\nWith cleaning stuff:')\n",
    "    #for i,j in nms.items():\n",
    "        #print(i, j)\n",
    "\n",
    "    nml = len(nms)\n",
    "    for i in range(nml):\n",
    "        nmk = len(nms[i])\n",
    "        for j in range(nmk):\n",
    "            nmt = clean_things(nms[i][j]) # nm_temp\n",
    "            #print(nmt)\n",
    "            if len(nmt) == 2:\n",
    "                nmk = len(nms[i])\n",
    "                nms[i][nmk] = nmt[1]\n",
    "                nms[i][j] = nmt[0]\n",
    "            elif len(nmt) == 1:\n",
    "                nms[i][j] = nmt[0]\n",
    "            elif len(nmt) == 0:\n",
    "                del nms[i][j]\n",
    "\n",
    "    #cleaning\n",
    "    for l,i in nms.items():\n",
    "        k = 0\n",
    "        new_dict = {}\n",
    "        for j in i:\n",
    "            new_dict[k] = i[j]\n",
    "            k += 1\n",
    "        nms[l] = new_dict\n",
    "\n",
    "\n",
    "    #print('\\nAfter re-indexing change:', nms)\n",
    "\n",
    "\n",
    "    def sp_nlp(nmi):\n",
    "        doc = nlp(nmi)\n",
    "        in_tok_n = [] #token is noun\n",
    "        tok_n = np.array(in_tok_n)\n",
    "        j = 0\n",
    "        deli = 0\n",
    "        for token in doc:\n",
    "            if token.pos_ == 'PROPN':\n",
    "                tok_n = np.append(tok_n, 1)\n",
    "            else:\n",
    "                #tok_n[j] = 0\n",
    "                tok_n = np.append(tok_n, 0)\n",
    "            j = j+1\n",
    "\n",
    "        if np.mean(tok_n) < 0.5:\n",
    "            #print('\\n', token.text, '\\n', token.pos_, '\\n', nmi)\n",
    "            deli = 1 #delete_item\n",
    "        return deli\n",
    "\n",
    "    del_ar = []\n",
    "\n",
    "    for m,i in nms.items():\n",
    "        for j,k in i.items():\n",
    "            deli = sp_nlp(k)\n",
    "            if deli == 1:\n",
    "                del_ar.append([m,j])\n",
    "\n",
    "\n",
    "    #print(del_ar)\n",
    "    for i in del_ar:\n",
    "        del nms[i[0]][i[1]]\n",
    "        ##print(i[0])\n",
    "\n",
    "    #print('\\nnms list after NLP:\\n')\n",
    "    #for i,j in nms.items():\n",
    "        #print(j)\n",
    "    ##print('\\nnms list after NLP:', nms, '\\n')\n",
    "    #print('nms list length after NLP:', len(nms), '\\n')\n",
    "\n",
    "\n",
    "#cleaning\n",
    "    for l,i in nms.items():\n",
    "        k = 0\n",
    "        new_dict = {}\n",
    "        for j in i:\n",
    "            new_dict[k] = i[j]\n",
    "            k += 1\n",
    "        nms[l] = new_dict\n",
    "\n",
    "    return nms\n",
    "\n",
    "#########################################\n",
    "\n",
    "all_pages = dill.load(open('link_soups.pkd', 'rb'))\n",
    "#all_pages = dill.load(open('notallpages.pkd', 'rb'))\n",
    "\n",
    "nmp = []\n",
    "\n",
    "for k in range(len(all_pages)):\n",
    "    wbsite = all_pages[k]\n",
    "    soup = BeautifulSoup(wbsite.text, 'lxml')\n",
    "    nms = graph3all(soup)\n",
    "\n",
    "    for i in nms.values():\n",
    "        nmj = list(itertools.combinations(i.values(), 2))\n",
    "        for j in nmj:\n",
    "            nmp.append(j)\n",
    "\n",
    "##print(missed_pages)\n",
    "dill.dump(nmp, open('nmp_new.pkd', 'wb'))\n",
    "\n",
    "#dill.dump(nms, open('nms1_list.pkd', 'wb'))\n",
    "# #dill.dump(nm_out2, open('p1_name_list.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping all of the pages could take 10 minutes or so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining analysis, we think of the problem in terms of a\n",
    "[network](http://en.wikipedia.org/wiki/Computer_network) or a\n",
    "[graph](https://en.wikipedia.org/wiki/Graph_%28discrete_mathematics%29).  Any time a pair of people appear in a photo together, that is considered a link.  What we have described is more appropriately called an (undirected)\n",
    "[multigraph](http://en.wikipedia.org/wiki/Multigraph) with no self-loops but this has an obvious analog in terms of an undirected [weighted graph](http://en.wikipedia.org/wiki/Graph_%28mathematics%29#Weighted_graph).  In this problem, we will analyze the social graph of the New York social elite.  We recommend using python's [`networkx`](https://networkx.github.io/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools  # itertools.combinations may be useful\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, you should end up with over 100,000 captions and more than 110,000 names, connected in about 200,000 pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: degree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest question to ask is \"who is the most popular\"?  The easiest way to answer this question is to look at how many connections everyone has.  Return the top 100 people and their degree.  Remember that if an edge of the graph has weight 2, it counts for 2 in the degree.\n",
    "\n",
    "**Checkpoint:** Some aggregate stats on the solution\n",
    "\n",
    "    \"count\": 100.0\n",
    "    \"mean\": 189.92\n",
    "    \"std\": 87.8053034454\n",
    "    \"min\": 124.0\n",
    "    \"25%\": 138.0\n",
    "    \"50%\": 157.0\n",
    "    \"75%\": 195.0\n",
    "    \"max\": 666.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "import operator\n",
    "\n",
    "#nms = dill.load(open('nms1_list.pkd', 'rb'))\n",
    "nmp1 = dill.load(open('nmp_new.pkd', 'rb'))\n",
    "nmp2 = dill.load(open('nmp_new_td.pkd', 'rb'))\n",
    "\n",
    "\n",
    "print('Size of nmp is:', len(nmp1))\n",
    "print('Size of nmp is:', len(nmp2))\n",
    "nmp = nmp1 + nmp2\n",
    "print('Size of nmp is:', len(nmp))\n",
    "\n",
    "\n",
    "#nmp = nmp[0:80000]\n",
    "G=nx.Graph()\n",
    "\n",
    "\n",
    "# for i in nmp:\n",
    "#     #print(i)\n",
    "#     if not 'Jr.' in i or 'M.D.' in i:\n",
    "#         if G.has_edge(i[0],i[1]):\n",
    "#             G.edges[i[0], i[1]]['weight'] += 1\n",
    "#             #print(G.edges[i[0], i[1]]['weight'])\n",
    "#         else:\n",
    "#             G.add_edge(*i, weight = 1)\n",
    "\n",
    "for i in nmp:\n",
    "    #print(i)\n",
    "    if G.has_edge(i[0],i[1]):\n",
    "        G.edges[i[0], i[1]]['weight'] += 1\n",
    "            #print(G.edges[i[0], i[1]]['weight'])\n",
    "    else:\n",
    "        G.add_edge(*i, weight = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Degree of a person (question 3)\n",
    "nd = G.degree\n",
    "sorted_nd = sorted(nd, key=operator.itemgetter(1))\n",
    "sorted_nd.reverse()\n",
    "print(sorted_nd[0:100])\n",
    "\n",
    "nd_50 = []\n",
    "for i in sorted_nd:\n",
    "    nd_50.append((i[0], int(i[1]*1.2)))\n",
    "print(nd_50[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq  # Heaps are efficient structures for tracking the largest\n",
    "              # elements in a collection.  Use introspection to find the\n",
    "              # function you need.\n",
    "degree = [('Alec Baldwin', 144)] * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: PageRank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar way to determine popularity is to look at their\n",
    "[PageRank](http://en.wikipedia.org/wiki/PageRank).  PageRank is used for web ranking and was originally\n",
    "[patented](http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=6285999) by Google and is essentially the stationary distribution of a [Markov\n",
    "chain](http://en.wikipedia.org/wiki/Markov_chain) implied by the social graph. You can implement this yourself or use the version in `networkx`\n",
    "\n",
    "Use 0.85 as the damping parameter so that there is a 15% chance of jumping to another vertex at random.\n",
    "\n",
    "**Checkpoint:** Some aggregate stats on the solution\n",
    "\n",
    "    \"count\": 100.0\n",
    "    \"mean\": 0.0001841088\n",
    "    \"std\": 0.0000758068\n",
    "    \"min\": 0.0001238355\n",
    "    \"25%\": 0.0001415028\n",
    "    \"50%\": 0.0001616183\n",
    "    \"75%\": 0.0001972663\n",
    "    \"max\": 0.0006085816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #PageRank (Question 4)\n",
    "# pr = nx.pagerank(G)\n",
    "# sorted_pr = sorted(pr.items(), key=operator.itemgetter(1))\n",
    "# sorted_pr.reverse()\n",
    "# print(sorted_pr[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank = [('Martha Stewart', 0.00019312108706213307)] * 100\n",
    "\n",
    "grader.score('graph__pagerank', pagerank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: best_friends\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting question is who tend to co-occur with each other.  Give us the 100 edges with the highest weights.\n",
    "\n",
    "Google these people and see what their connection is.  Can we use this to detect instances of infidelity?\n",
    "\n",
    "**Checkpoint:** Some aggregate stats on the solution\n",
    "\n",
    "    \"count\": 100.0\n",
    "    \"mean\": 25.84\n",
    "    \"std\": 16.0395470855\n",
    "    \"min\": 14.0\n",
    "    \"25%\": 16.0\n",
    "    \"50%\": 19.0\n",
    "    \"75%\": 29.25\n",
    "    \"max\": 109.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5\n",
    "# ct = 0\n",
    "# bf = []\n",
    "# for (u,v,d) in G.edges(data='weight'):\n",
    "#     if d >9:\n",
    "#         #print(u, v, d)\n",
    "#         bf.append(((u, v), d))\n",
    "#         ct += 1\n",
    "#\n",
    "# sorted_bf = sorted(bf, key=operator.itemgetter(1))\n",
    "# sorted_bf.reverse()\n",
    "# print(sorted_bf[3:103])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_friends = [(('Michael Kennedy', 'Eleanora Kennedy'), 41)] * 100\n",
    "\n",
    "grader.score('graph__best_friends', best_friends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
